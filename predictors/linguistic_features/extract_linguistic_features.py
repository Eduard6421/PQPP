# -*- coding: utf-8 -*-
"""Copie a blocnotesului linguistic_features.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/149IOkSFbTiSkJw1v3ZAMcZTucl-iP-5Q
"""

from nltk.tokenize import word_tokenize, sent_tokenize
import nltk
import re
from nltk.corpus import words as nltk_words
import string
import pandas as pd
import spacy
import pickle
import scipy.stats
from nltk.corpus import stopwords
from nltk.corpus import wordnet as wn

nltk.download("punkt")
nltk.download("averaged_perceptron_tagger")
nltk.download("stopwords")
nltk.download("wordnet")


def load_dataset(file_path):
    with open(file_path, "rb") as file:
        data = pickle.load(file)
    return data


def split_data(data):

    # Calculate the indices for splitting
    train_size = int(len(data) * 0.6)
    validation_size = int(len(data) * 0.2)

    # Split the data
    train_data = data[:train_size]
    validation_data = data[train_size : train_size + validation_size]
    test_data = data[train_size + validation_size :]

    return test_data


def calculate_correlations(list1, list2):
    # Check if the lists are of the same length
    if len(list1) != len(list2):
        return "The lists are not of the same length"

    # Calculate Pearson correlation
    pearson_corr, pvaluep = scipy.stats.pearsonr(list1, list2)

    # Calculate Kendall Tau correlation
    kendall_corr, pvalue = scipy.stats.kendalltau(list1, list2)

    return pearson_corr, pvaluep, kendall_corr, pvalue


def calculate_all_correlations(list_to_corelate, *lists):
    results = {}
    list_names = [
        "count_synsets",
        "count_words",
        "avg_word_length",
        "average_proper_nouns",
        "average_number_of_acronyms",
        "average_numerals_occurrence",
        "average_conjunctions_per_word",
        "average_prepositions_per_word",
    ]

    for list_name, data_list in zip(list_names, lists):
        correlation_result = calculate_correlations(list_to_corelate, data_list)
        results[list_name] = correlation_result

    return results


def count_synsets(sentence):
    # Tokenize the sentence into words
    words = nltk.word_tokenize(sentence)
    stop_words = set(stopwords.words("english"))

    word_synsets = {}

    # Dictionary to hold word and its synset count
    synset_count = {}
    total_synsets = 0

    # For each word, find the number of synsets and store it in the dictionary
    for word in words:
        if word.lower() not in stop_words:
            synsets = wn.synsets(word)
            synset_names = [synset.name() for synset in synsets]
            word_synsets[word] = synset_names
            synset_count[word] = len(synsets)
            total_synsets += len(synsets)

    return total_synsets


def count_words(sentence):
    words = word_tokenize(sentence)
    words = [word for word in words if word not in string.punctuation]
    number_of_words = len(words)
    return number_of_words


def avg_word_length(sentence):
    words = word_tokenize(sentence)
    words = [word for word in words if word not in string.punctuation]
    words = sentence.split()
    num_words = len(words)
    avg_word_length = (
        sum(len(word) for word in words) / num_words if num_words > 0 else 0
    )
    return avg_word_length


def average_proper_nouns(text):
    words = word_tokenize(text)
    words = [word for word in words if word not in string.punctuation]
    tagged_words = nltk.pos_tag(words)

    # Count 'NNP' and 'NNPS' tags (proper nouns)
    proper_noun_count = sum(tag in ["NNP", "NNPS"] for _, tag in tagged_words)

    # Calculate the average
    average = proper_noun_count / len(words) if words else 0
    return average


def average_number_of_acronyms(text):
    """
    Calculate and return the average number of acronyms per sentence in the given text.
    An acronym is defined here as a consecutive sequence of capital letters.
    """
    # Tokenize the text into sentences
    words = word_tokenize(text)
    words = [word for word in words if word not in string.punctuation]

    total_acronyms = 0

    # Process each sentence
    for sentence in words:
        # Use a regular expression to find all acronyms in the sentence
        acronyms = re.findall(r"\b[A-Z]{2,}\b", sentence)
        total_acronyms += len(acronyms)

    # Calculate the average number of acronyms per sentence
    average_acronyms = total_acronyms / len(words) if words else 0
    return average_acronyms


def average_numerals_occurrence(text):
    # Tokenize the text into sentences
    words = word_tokenize(text)
    words = [word for word in words if word not in string.punctuation]

    # Regular expression for numeral values (including dates and quantities)
    numeral_pattern = r"\b\d{4}-\d{2}-\d{2}\b|\b\d{2}/\d{2}/\d{4}\b|\b\d+\b"

    # Count the occurrence of numeral values in each sentence
    numeral_counts = [len(re.findall(numeral_pattern, word)) for word in words]

    # Calculate the average occurrence of numerals per sentence
    average_occurrence = sum(numeral_counts) / len(words) if words else 0

    return average_occurrence


def average_conjunctions_per_word(text):
    # Tokenize the text into words, removing punctuation
    words = word_tokenize(text)
    words = [word.lower() for word in words if word not in string.punctuation]

    # Prepare a set of English conjunctions
    english_conjunctions = set(["for", "and", "nor", "but", "or", "yet", "so"])

    # Count the total number of conjunctions
    conjunction_count = sum(word in english_conjunctions for word in words)

    # Calculate the average occurrence of conjunctions per word
    average_occurrence = conjunction_count / len(words) if words else 0

    return average_occurrence


def average_prepositions_per_word(text):
    words = word_tokenize(text)
    words = [word.lower() for word in words if word not in string.punctuation]

    # Prepare a set of English prepositions
    english_prepositions = set(
        [
            "aboard",
            "about",
            "above",
            "across",
            "after",
            "against",
            "along",
            "amid",
            "among",
            "around",
            "as",
            "at",
            "before",
            "behind",
            "below",
            "beneath",
            "beside",
            "between",
            "beyond",
            "by",
            "concerning",
            "considering",
            "despite",
            "down",
            "during",
            "except",
            "for",
            "from",
            "in",
            "inside",
            "into",
            "like",
            "near",
            "of",
            "off",
            "on",
            "onto",
            "out",
            "outside",
            "over",
            "past",
            "regarding",
            "round",
            "since",
            "through",
            "to",
            "toward",
            "towards",
            "under",
            "underneath",
            "unlike",
            "until",
            "up",
            "upon",
            "with",
            "within",
            "without",
        ]
    )

    preposition_count = sum(word in english_prepositions for word in words)

    average_occurrence = preposition_count / len(words) if words else 0

    return average_occurrence


avg_scores_p10_df = load_dataset("../../../dataset/avg_scores_p10_new.pickle")
avg_scores_mrr_df = load_dataset("../../../dataset/avg_scores_mrr_new.pickle")
avg_scores_p10_test = split_data(list(avg_scores_p10_df.values()))
avg_scores_mrr_test = split_data(list(avg_scores_mrr_df.values()))

best_captions_df = load_dataset("../../../dataset/best_captions_df.pickle")
best_captions_df = best_captions_df.head(10000)
best_captions_test = split_data(best_captions_df)

gt_for_generative_all_df = pd.read_csv(
    "../../../dataset/gt_for_generative_all_models_new.csv"
)
gt_for_generative_all_list = gt_for_generative_all_df["score"].to_list()
gt_for_generative_all_test = split_data(gt_for_generative_all_list)

best_captions_test["count_synsets"] = best_captions_test["best_caption"].apply(
    count_synsets
)
best_captions_test["count_words"] = best_captions_test["best_caption"].apply(
    count_words
)
best_captions_test["avg_word_length"] = best_captions_test["best_caption"].apply(
    avg_word_length
)


best_captions_test["average_proper_nouns"] = best_captions_test["best_caption"].apply(
    average_proper_nouns
)
best_captions_test["average_number_acronyms"] = best_captions_test[
    "best_caption"
].apply(average_number_of_acronyms)
best_captions_test["average_numerals_occurrence"] = best_captions_test[
    "best_caption"
].apply(average_numerals_occurrence)
best_captions_test["average_conjunctions_per_word"] = best_captions_test[
    "best_caption"
].apply(average_conjunctions_per_word)
best_captions_test["average_prepositions_per_word"] = best_captions_test[
    "best_caption"
].apply(average_prepositions_per_word)

correlation_results_p10 = calculate_all_correlations(
    avg_scores_p10_test,
    best_captions_test["count_synsets"],
    best_captions_test["count_words"],
    best_captions_test["avg_word_length"],
    best_captions_test["average_proper_nouns"].to_list(),
    best_captions_test["average_number_acronyms"].to_list(),
    best_captions_test["average_numerals_occurrence"].to_list(),
    best_captions_test["average_conjunctions_per_word"].to_list(),
    best_captions_test["average_prepositions_per_word"].to_list(),
)

print(correlation_results_p10)

correlation_results_mrr = calculate_all_correlations(
    avg_scores_mrr_test,
    best_captions_test["average_proper_nouns"].to_list(),
    best_captions_test["average_number_acronyms"].to_list(),
    best_captions_test["average_numerals_occurrence"].to_list(),
    best_captions_test["average_conjunctions_per_word"].to_list(),
    best_captions_test["average_prepositions_per_word"].to_list(),
)

print(correlation_results_mrr)

correlation_results_gt = calculate_all_correlations(
    gt_for_generative_all_test,
    best_captions_test["count_synsets"],
    best_captions_test["count_words"],
    best_captions_test["avg_word_length"],
    best_captions_test["average_proper_nouns"].to_list(),
    best_captions_test["average_number_acronyms"].to_list(),
    best_captions_test["average_numerals_occurrence"].to_list(),
    best_captions_test["average_conjunctions_per_word"].to_list(),
    best_captions_test["average_prepositions_per_word"].to_list(),
)

print(correlation_results_gt)

gt_for_generative_all_test
